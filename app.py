from flask import Flask, jsonify, render_template, request
import google.generativeai as genai
import requests
from pinecone.grpc import PineconeGRPC as Pinecone

################################################################################################
# Import necessary libraries

# Configure the GenAI API
genai.configure(api_key='AIzaSyBgEWO0_xIuVPUWDQuQVvs8v3KtVHJY-7s')

qroq_api="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"

PINECONE_API_KEY="ef7da690-c8fa-4156-8bfe-16292e8c7b5d"

from langchain_groq import ChatGroq

llm = ChatGroq(
    temperature=0, 
    groq_api_key='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', 
    model_name="llama-3.1-70b-versatile"
)

# response = llm.invoke("The first person to land on moon was ...")
# print(response.content)
# Initialize the Gemini model
model = genai.GenerativeModel('gemini-1.5-flash')

pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "sharia-compliant"

# Define function to get query embedding
def get_query_embedding(query: str):
    query_embedding = pc.inference.embed(
    model="multilingual-e5-large",
    inputs=[query],
    parameters={
        "input_type": "query"
    }
)
    return query_embedding

def filter_results(query_embedding):
    index = pc.Index(index_name)

    results = index.query(
    namespace="sharia-namespace",
    vector=query_embedding[0].values,
    top_k=1,
    include_values=True,
    include_metadata=True
)
    return results['matches'][0]['metadata']['text']

# Define function to search Pinecone index
def search_results(question):
    query_embedding=get_query_embedding(question)

    return filter_results(query_embedding)






# Function to ask a question based on file content
def ask_question(question):
    
    final_data=search_results(question)
    print('\n','-'*80,'\n',final_data,'\n','-'*80,'\n')
    new_prompt=f"""
        data: {final_data[3:]} 
        quesiton : {question}

        if quesiton is related to {final_data} then provide a detailed answer given formet. 


        formet = 
            - first tow line in arabinc.
            - space
            - english. 
            
        you also add some extra information but not more.

        ###
        For general greetings (like "hi", "hello", "how are you" ect):
        this time ignore data. and all and give Respond with a natural conversational answer without any formatting.

        If question is out of tha data then answer me on your information. using formet.
        ###

    """
    response = llm.invoke(new_prompt)
    # answer=model.generate_content(new_prompt)
    # Return the answer generated by the model
    return response.content






   
   
app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/ask_question', methods=['POST'])
def answer():
    data = request.get_json()
    question = data.get('question', '')
    
    # Example response; replace with your logic
    response =ask_question(question)
    return jsonify({'response': response})

if __name__ == '__main__':
    app.run(debug=True)

